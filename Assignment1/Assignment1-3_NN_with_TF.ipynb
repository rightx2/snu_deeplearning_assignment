{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #1 Part 3: Playing with Neural Networks by TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jaehee Jang, September 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Previously in `Assignment2-1_Data_Curation.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **part 1 - 3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a compressed file called *[Your student number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; 20\\*\\*-\\*\\*\\*\\*\\*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "\n",
    "First reload the data we generated in `Assignment2-1_Data_Curation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    \n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    \n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "## TensorFlow tutorial: Fully Connected Network\n",
    "\n",
    "We're first going to train a **fully connected network** with *1 hidden layer* with *1024 units* using stochastic gradient descent (SGD).\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nn_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, nn_hidden]))\n",
    "    b1 = tf.Variable(tf.zeros([nn_hidden]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([nn_hidden, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    hidden = tf.tanh(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "    logits = tf.matmul(hidden, w2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    )\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    valid_hidden = tf.tanh(tf.matmul(tf_valid_dataset, w1) + b1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, w2) + b2)\n",
    "\n",
    "    test_hidden = tf.tanh(tf.matmul(tf_test_dataset, w1) + b1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden, w2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 40.604282\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 23.7%\n",
      "Minibatch loss at step 1000: 5.476233\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 2000: 2.062078\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 3000: 2.032331\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 4000: 0.917977\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 5000: 0.825229\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 6000: 0.455504\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 7000: 1.409604\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 8000: 0.948712\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 9000: 0.327592\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" %\n",
    "                  accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "        test_prediction.eval(), test_labels))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, \"./model_checkpoints/my_model_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.kr/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "1. Experiment with different hyperparameters: num_steps, learning rate, etc.\n",
    "2. We used a fixed learning rate $\\epsilon$ for gradient descent. Implement an annealing schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). *Hint*. Try using `tf.train.exponential_decay`.    \n",
    "3. We used a $\\tanh$ activation function for our hidden layer. Experiment with other activation functions included in TensorFlow.\n",
    "4. Extend the network to multiple hidden layers. Experiment with the layer sizes. Adding another hidden layer means you will need to adjust the code. \n",
    "5. Introduce and tune regularization method (e.g. L2 regularization) for your model. Remeber that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should imporve your validation / test accuracy.\n",
    "6. Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "**Evaluation:** Rank by test accuracy. If we have ties, we will test with another dataset which is not included in notMnist dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "   Minibatch loss : 3.7817182540893555\n",
      "   Minibatch accuracy : 17.1875\n",
      "   Validation accuracy : 22.13\n",
      "3000\n",
      "   Minibatch loss : 1.053476333618164\n",
      "   Minibatch accuracy : 88.28125\n",
      "   Validation accuracy : 89.48\n",
      "6000\n",
      "   Minibatch loss : 0.4979506731033325\n",
      "   Minibatch accuracy : 97.65625\n",
      "   Validation accuracy : 90.37\n",
      "9000\n",
      "   Minibatch loss : 0.3790864646434784\n",
      "   Minibatch accuracy : 98.4375\n",
      "   Validation accuracy : 90.93\n",
      "12000\n",
      "   Minibatch loss : 0.340098112821579\n",
      "   Minibatch accuracy : 97.65625\n",
      "   Validation accuracy : 90.71\n",
      "15000\n",
      "   Minibatch loss : 0.2729608118534088\n",
      "   Minibatch accuracy : 97.65625\n",
      "   Validation accuracy : 90.92\n",
      "18000\n",
      "   Minibatch loss : 0.21959781646728516\n",
      "   Minibatch accuracy : 100.0\n",
      "   Validation accuracy : 91.27\n",
      "21000\n",
      "   Minibatch loss : 0.25970157980918884\n",
      "   Minibatch accuracy : 96.875\n",
      "   Validation accuracy : 91.65\n",
      "24000\n",
      "   Minibatch loss : 0.2022579163312912\n",
      "   Minibatch accuracy : 99.21875\n",
      "   Validation accuracy : 91.51\n",
      "27000\n",
      "   Minibatch loss : 0.19867537915706635\n",
      "   Minibatch accuracy : 98.4375\n",
      "   Validation accuracy : 91.87\n",
      "30000\n",
      "   Minibatch loss : 0.18112604320049286\n",
      "   Minibatch accuracy : 99.21875\n",
      "   Validation accuracy : 91.87\n",
      "33000\n",
      "   Minibatch loss : 0.22143733501434326\n",
      "   Minibatch accuracy : 97.65625\n",
      "   Validation accuracy : 91.79\n",
      "36000\n",
      "   Minibatch loss : 0.19840413331985474\n",
      "   Minibatch accuracy : 97.65625\n",
      "   Validation accuracy : 92.15\n",
      "39000\n",
      "   Minibatch loss : 0.16143108904361725\n",
      "   Minibatch accuracy : 100.0\n",
      "   Validation accuracy : 92.1\n",
      "42000\n",
      "   Minibatch loss : 0.16131329536437988\n",
      "   Minibatch accuracy : 100.0\n",
      "   Validation accuracy : 91.94\n",
      "45000\n",
      "   Minibatch loss : 0.1636337786912918\n",
      "   Minibatch accuracy : 100.0\n",
      "   Validation accuracy : 92.09\n",
      "48000\n",
      "   Minibatch loss : 0.16478244960308075\n",
      "   Minibatch accuracy : 100.0\n",
      "   Validation accuracy : 92.0\n",
      "Test accuracy: 96.85\n"
     ]
    }
   ],
   "source": [
    "condition_dict = [\n",
    "    {\"batch_size\":128,\n",
    "     \"layer1_size\":4096,\n",
    "     \"layer2_size\":2048,\n",
    "     \"layer3_size\":128,\n",
    "     \"beta\":0.0003,\n",
    "     \"stddev\":2,\n",
    "     \"dropout\":False,\n",
    "     \"learning_rate\":0.5,\n",
    "     \"decay_steps\":4000,\n",
    "     \"decay_rate\":0.7,\n",
    "     \"num_steps\": 50000},\n",
    "#     {\n",
    "#         \"batch_size\":128, \n",
    "#         \"layer1_size\":4096, \n",
    "#         \"layer2_size\":2048, \n",
    "#         \"layer3_size\":128, \n",
    "#         \"beta\":0.0003, \n",
    "#         \"stddev\":2, \n",
    "#         \"dropout\":False, \n",
    "#         \"learning_rate\":0.3, \n",
    "#         \"decay_steps\":4000, \n",
    "#         \"decay_rate\":0.7, \n",
    "#         \"num_steps\": 50000\n",
    "#     },\n",
    "]\n",
    "\n",
    "for condition in condition_dict:\n",
    "    batch_size = condition[\"batch_size\"]\n",
    "    layer1_size = condition[\"layer1_size\"]\n",
    "    layer2_size = condition[\"layer2_size\"]\n",
    "    layer3_size = condition[\"layer3_size\"]\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        train_x = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "        train_y = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "        valid_set = tf.constant(valid_dataset)\n",
    "        test_set = tf.constant(test_dataset)\n",
    "\n",
    "        reg_param = tf.placeholder(tf.float32)\n",
    "        global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "        W1 = tf.Variable(tf.truncated_normal([image_size * image_size, layer1_size], stddev=np.sqrt(condition[\"stddev\"] / (image_size * image_size))))\n",
    "        b1 = tf.Variable(tf.zeros([layer1_size]))\n",
    "\n",
    "        W2 = tf.Variable(tf.truncated_normal([layer1_size, layer2_size], stddev=np.sqrt(condition[\"stddev\"] / layer1_size)))\n",
    "        b2 = tf.Variable(tf.zeros([layer2_size]))\n",
    "\n",
    "        W3 = tf.Variable(tf.truncated_normal([layer2_size, layer3_size], stddev=np.sqrt(condition[\"stddev\"] / layer2_size)))\n",
    "        b3 = tf.Variable(tf.zeros([layer3_size]))\n",
    "\n",
    "        W4 = tf.Variable(tf.truncated_normal([layer3_size, num_labels], stddev=np.sqrt(condition[\"stddev\"] / layer3_size)))\n",
    "        b4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        y1 = tf.nn.relu(tf.matmul(train_x, W1) + b1)\n",
    "        if condition[\"dropout\"]:\n",
    "            y1 = tf.nn.dropout(y1, 0.5)\n",
    "\n",
    "        y2 = tf.nn.relu(tf.matmul(y1, W2) + b2)\n",
    "        if condition[\"dropout\"]:\n",
    "            y2 = tf.nn.dropout(y2, 0.5)\n",
    "\n",
    "        y3 = tf.nn.relu(tf.matmul(y2, W3) + b3)\n",
    "        if condition[\"dropout\"]:\n",
    "            y3 = tf.nn.dropout(y3, 0.5)\n",
    "\n",
    "        logits = tf.matmul(y3, W4) + b4\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=train_y)\n",
    "        )\n",
    "\n",
    "        loss = loss + reg_param * (\n",
    "            tf.nn.l2_loss(W1)+\n",
    "            tf.nn.l2_loss(b1)+\n",
    "            tf.nn.l2_loss(W2)+\n",
    "            tf.nn.l2_loss(b2)+\n",
    "            tf.nn.l2_loss(W3)+\n",
    "            tf.nn.l2_loss(b3)+\n",
    "            tf.nn.l2_loss(W4)+\n",
    "            tf.nn.l2_loss(b4)\n",
    "        )\n",
    "\n",
    "        # Optimizer\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            condition[\"learning_rate\"],\n",
    "            global_step,\n",
    "            condition[\"decay_steps\"],\n",
    "            condition[\"decay_rate\"],\n",
    "            staircase=True\n",
    "        )\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        a1_valid = tf.nn.relu(tf.matmul(valid_set, W1) + b1)\n",
    "        a2_valid = tf.nn.relu(tf.matmul(a1_valid, W2) + b2)\n",
    "        a3_valid = tf.nn.relu(tf.matmul(a2_valid, W3) + b3)\n",
    "        valid_logits = tf.matmul(a3_valid, W4) + b4\n",
    "        valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "        a1_test = tf.nn.relu(tf.matmul(test_set, W1) + b1)\n",
    "        a2_test = tf.nn.relu(tf.matmul(a1_test, W2) + b2)\n",
    "        a3_test = tf.nn.relu(tf.matmul(a2_test, W3) + b3)\n",
    "        test_logits = tf.matmul(a3_test, W4) + b4\n",
    "        test_prediction = tf.nn.softmax(test_logits)\n",
    "\n",
    "    num_steps = condition[\"num_steps\"]\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "    with tf.Session(graph=graph, config=config) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps + 1):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction],\n",
    "                feed_dict={\n",
    "                    train_x: batch_data,\n",
    "                    train_y: batch_labels,\n",
    "                    reg_param: condition[\"beta\"],\n",
    "                }\n",
    "            )\n",
    "            if (step % 3000 == 0):\n",
    "                print(step)\n",
    "                print(\"   Minibatch loss : {}\".format(l))\n",
    "                print(\"   Minibatch accuracy : {}\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"   Validation accuracy : {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "        print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(), test_labels)))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, \"./model_checkpoints/my_model_final\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
